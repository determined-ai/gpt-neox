# Copyright (c) 2021, EleutherAI contributors
# This file is based on code by the authors denoted below and has been modified from its original version.
#
# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Train"""
from megatron.neox_arguments import NeoXArgs
from megatron.training import pretrain
import time
import glob
import shutil
import os

import determined as det


def wget_file(url: str, file: str) -> None:
    os.system(f"wget -nc {url} -O {file}")


def copy_hostfile(shared_fs_path: str, trial_id: int) -> str:
    # Copy the hostfile generated by the determined launcher to a fixed location on shared_fs.
    # TODO: Make this file actually temporary, i.e. delete when trial terminates?
    shared_hostfile = os.path.join(shared_fs_path, f"tmp/hostfile-{trial_id}.txt")
    if os.environ["RANK"] == "0":
        hostfile = glob.glob("/tmp/hostfile*.txt")[0]
        shutil.copyfile(hostfile, shared_hostfile)

    while not os.path.exists(shared_hostfile):
        time.sleep(1)

    return shared_hostfile


def download_enwik8(shared_fs_path: str):
    # Temporary helper for downloading enwik8 for testing.
    vocab_file = os.path.join(shared_fs_path, "gpt2-vocab.json")
    merges_file = os.path.join(shared_fs_path, "gpt2-merges.txt")
    wget_file("https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json", vocab_file)
    wget_file("https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt", merges_file)

    if os.environ["RANK"] == "0":
        os.system(f"python prepare_data.py -d {os.path.join(shared_fs_path, 'data')}")


if __name__ == "__main__":
    cluster_info = det.get_cluster_info()
    hparams = cluster_info.trial.hparams
    assert (
        "shared_fs_path" in hparams
    ), "Must provide shared_fs_path in hyperparameters (path to a filesystem shared by all processes)."
    shared_fs_path = cluster_info.trial.hparams["shared_fs_path"]
    if "download_enwik8" in hparams and hparams["download_enwik8"]:
        download_enwik8(shared_fs_path)
    shared_hostfile = copy_hostfile(shared_fs_path, cluster_info.trial.trial_id)
    assert (
        "config_files" in hparams
    ), "Must provide config_files in hyperparameters (list of paths to GPT-NeoX configuration yaml files)."
    config_files = cluster_info.trial.hparams["config_files"]
    overwrite_values = cluster_info.trial.hparams.get("overwrite_values", {})
    overwrite_values.update({"hostfile": shared_hostfile})
    if cluster_info.latest_checkpoint:
        # Resuming/continuing from Determined checkpoint, disable options that block loading of optimizer states and RNG.
        overwrite_values.update({"finetune": False, "no_load_optim": False, "no_load_rng": False})

    neox_args = NeoXArgs.from_ymls(
        config_files,
        overwrite_values=overwrite_values,
    )
    neox_args.configure_distributed_args()
    neox_args.build_tokenizer()  # tokenizer needs to be build in training in order to set the padding vocab
    neox_args.initialize_tensorboard_writer()  # is initialized if tensorboard directory is defined
    with det.core.init(distributed=det.core.DistributedContext.from_deepspeed()) as core_context:
        pretrain(neox_args=neox_args, core_context=core_context)
